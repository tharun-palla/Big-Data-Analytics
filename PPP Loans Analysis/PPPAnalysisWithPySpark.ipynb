{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tharunpalla8466/Big-Data-Analytics/blob/main/PPP%20Loans%20Analysis/PPPAnalysisWithPySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA with PySpark"
      ],
      "metadata": {
        "id": "ebTv_67Xec98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "WztDXE3lngQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of PPP loans data with demographics information.\n",
        "\n",
        "Now let's go tackle the PPP dataset and find some interesting trends using PySpark!"
      ],
      "metadata": {
        "id": "-4Km20MQgIWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download PPP data"
      ],
      "metadata": {
        "id": "aNh1_VfSfNjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the data from here (https://data.sba.gov/dataset/ppp-foia). We will use the first three files.\n",
        "\n",
        "I have tried copying the link address and pasting it here (so no need to mount your Google Drive) - but the link address changes every few months, so instead we can download locally, drag to Google Drive, and use a shareable link."
      ],
      "metadata": {
        "id": "iT_jRh90egQd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fE3CH1V2eDJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b77a612-0a99-4123-8703-aa3f4de75227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://data.sba.gov/dataset/8aa276e2-6cab-4f86-aca4-a7dde42adf24/resource/2b55e11d-7e75-4bbb-b526-69a06c0c4731/download/public_150k_plus_230101.csv\n",
            "To: /content/public_150k_plus_230101.csv\n",
            "100% 452M/452M [00:04<00:00, 101MB/s]\n",
            "Downloading...\n",
            "From: https://data.sba.gov/dataset/8aa276e2-6cab-4f86-aca4-a7dde42adf24/resource/5f700a26-02f9-4d97-94a3-e3c2c43871eb/download/public_up_to_150k_1_230101.csv\n",
            "To: /content/public_up_to_150k_1_230101.csv\n",
            "100% 414M/414M [00:06<00:00, 60.1MB/s]\n",
            "Downloading...\n",
            "From: https://data.sba.gov/dataset/8aa276e2-6cab-4f86-aca4-a7dde42adf24/resource/b785dfac-7d99-4bc0-9ab2-e87fe855174e/download/public_up_to_150k_2_230101.csv\n",
            "To: /content/public_up_to_150k_2_230101.csv\n",
            "100% 412M/412M [00:04<00:00, 102MB/s]\n"
          ]
        }
      ],
      "source": [
        "# download the SBA data\n",
        "# would be great but the link changes every quarter!\n",
        "\n",
        "\n",
        "!gdown https://data.sba.gov/dataset/8aa276e2-6cab-4f86-aca4-a7dde42adf24/resource/2b55e11d-7e75-4bbb-b526-69a06c0c4731/download/public_150k_plus_230101.csv\n",
        "!gdown https://data.sba.gov/dataset/8aa276e2-6cab-4f86-aca4-a7dde42adf24/resource/5f700a26-02f9-4d97-94a3-e3c2c43871eb/download/public_up_to_150k_1_230101.csv\n",
        "!gdown https://data.sba.gov/dataset/8aa276e2-6cab-4f86-aca4-a7dde42adf24/resource/b785dfac-7d99-4bc0-9ab2-e87fe855174e/download/public_up_to_150k_2_230101.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YbNxESZDpxo",
        "outputId": "f479a0dd-77db-4931-d759-103f374c167a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you look left, you should be able to see the three big .csv files on the lefthand side."
      ],
      "metadata": {
        "id": "miskYYfBeu45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download zipcode-level socioeconomic data\n",
        "It's generally not known what census tract a business resides in, but the zipcode is almost always known. \n",
        "\n",
        "This repo (https://github.com/Ro-Data/Ro-Census-Summaries-By-Zipcode) has zipcode-level census attributes. I have downloaded all of the files to our local runtime on the left."
      ],
      "metadata": {
        "id": "zcF03DIPfPqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/econ.txt\n",
        "!gdown https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/demo.txt\n",
        "!gdown https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/housing.txt\n",
        "!gdown https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/rural_urban.txt\n",
        "!gdown https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/social.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97iQRyaofvlD",
        "outputId": "c43eebd8-4ce1-4e60-a650-a7babf05565a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/econ.txt\n",
            "To: /content/econ.txt\n",
            "49.7MB [00:00, 111MB/s]\n",
            "Downloading...\n",
            "From: https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/demo.txt\n",
            "To: /content/demo.txt\n",
            "26.7MB [00:00, 77.7MB/s]\n",
            "Downloading...\n",
            "From: https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/housing.txt\n",
            "To: /content/housing.txt\n",
            "49.0MB [00:00, 98.0MB/s]\n",
            "Downloading...\n",
            "From: https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/rural_urban.txt\n",
            "To: /content/rural_urban.txt\n",
            "923kB [00:00, 21.0MB/s]       \n",
            "Downloading...\n",
            "From: https://raw.githubusercontent.com/Ro-Data/Ro-Census-Summaries-By-Zipcode/master/social.txt\n",
            "To: /content/social.txt\n",
            "46.0MB [00:00, 114MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Spark"
      ],
      "metadata": {
        "id": "l0Riiafgg0Ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H5Eqa_wqGE7c"
      },
      "outputs": [],
      "source": [
        "# install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "#!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz # update this!\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "#!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz # update this!\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\" # update this!\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "\n",
        "# findspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start session"
      ],
      "metadata": {
        "id": "CRxEW8DkGXCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start the builder pattern `SparkSession.builder` and then chain a configuration parameter that defined the application name."
      ],
      "metadata": {
        "id": "JuJ0P_fPhMro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Providing a useful `appName` helps you identify which programs are running on your Spark cluster."
      ],
      "metadata": {
        "id": "J2w5YYf2hsUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .appName(\"Project1_EDA_with_PySpark\")\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "O3AIwdPaGWTy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for concise script, we import all functions as F\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "-UEZxn3yBxNQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Create a folder on the local runtime called 'census' and copy all of the .txt files over to this folder"
      ],
      "metadata": {
        "id": "4QmlwI_Khtco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a directory called census\n",
        "!pwd \"/content\"\n",
        "! mkdir \"census\""
      ],
      "metadata": {
        "id": "woUFusuGh_tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f01681-50e5-4791-f8d8-07395d02467a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move all of the txt files we just downloaded\n",
        "! mv demo.txt social.txt econ.txt housing.txt rural_urban.txt \"census\" "
      ],
      "metadata": {
        "id": "NFm3e40Gj2Pb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Create a folder on the local runtime called 'PPP' and copy all of the .csv files over to this folder\n",
        "Like we did in class! Use mv and not cp."
      ],
      "metadata": {
        "id": "tsO8jVgy8Vna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a directory called PPP\n",
        "! mkdir \"PPP\""
      ],
      "metadata": {
        "id": "Qu0_pDlrnJKF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move all of the csv files we just downloaded\n",
        "! mv public_150k_plus_230101.csv public_up_to_150k_1_230101.csv public_up_to_150k_2_230101.csv \"PPP\""
      ],
      "metadata": {
        "id": "c4zTna5onGea"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 Clean the PPP data "
      ],
      "metadata": {
        "id": "rE34nkW2EcGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Append/concatenate all of the PPP csv files together\n",
        "You should end up with ~2.76M rows and 53 columns if you did it right. You only need one line of code if you use the wildcard. Make sure you print the rows and column count to check your work or points off!"
      ],
      "metadata": {
        "id": "NOSbkyu5pCpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pppDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/content/PPP/*\")\n",
        "print((pppDF.count(), len(pppDF.columns)))"
      ],
      "metadata": {
        "id": "zUh8hjEE4Z_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pppDF.show(10)"
      ],
      "metadata": {
        "id": "Rty9YTF878UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Clean the zipcode column for PPP files\n",
        "There are some that are missing and there are some that have a long format (12345-6789).\n",
        "\n",
        "We only want records where the `BorrowerZip`:\n",
        "*  zip code exists (is non-null)\n",
        "*  if it does exist, make return only the first 5 digits (https://www.datasciencemadesimple.com/extract-first-n-and-last-n-character-in-pyspark/)\n",
        "\n",
        "Now answer:\n",
        "* How many rows did you drop? \n",
        "* What percentage of the original data was returned? Do you think this is acceptable data quality?"
      ],
      "metadata": {
        "id": "MGdzCWWXqYbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pppDF = pppDF.withColumn(\"BorrowerZip\",pppDF.BorrowerZip.substr(1,5)).dropna(subset=[\"BorrowerZip\"])\n",
        "pppDF.show(10)\n",
        "print((pppDF.count(), len(pppDF.columns)))"
      ],
      "metadata": {
        "id": "8HPbdTLc-wdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We dropped 160 rows and that is 0.057%.\n",
        "The data quality acceptable."
      ],
      "metadata": {
        "id": "hVtuuPP6Hq43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 Clean up ALL census files\n",
        "Try to use as little code as possible. You should end up with exactly 33120 rows and ~900 columns (depending on if you dropped the dirty zip code column.)"
      ],
      "metadata": {
        "id": "-dtzAgMW8rjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Read the files"
      ],
      "metadata": {
        "id": "lGJsqYfMqkTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "customSchema = StructType([\n",
        "    StructField(\"ZCTA5\", StringType(), True)])"
      ],
      "metadata": {
        "id": "JzJGyyN0fmka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipcodesDF = spark.read.options(delimiter=\"\\t\").schema(customSchema).option(\"header\", \"true\").csv(\"/content/census/demo.txt\")\n",
        "demoDF = spark.read.options(delimiter=\"\\t\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/content/census/demo.txt\")\n",
        "econDF = spark.read.options(delimiter=\"\\t\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/content/census/econ.txt\")\n",
        "housingDF = spark.read.options(delimiter=\"\\t\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/content/census/housing.txt\")\n",
        "ruralUrbanDF = spark.read.options(delimiter=\"\\t\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/content/census/rural_urban.txt\")\n",
        "socialDF = spark.read.options(delimiter=\"\\t\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/content/census/social.txt\")"
      ],
      "metadata": {
        "id": "pCIrUsTYoNaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipcodesDF.printSchema()"
      ],
      "metadata": {
        "id": "Jyxv_BJhWcE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Join files together"
      ],
      "metadata": {
        "id": "AoHYlnTxqy9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: everything except the zipcode column should be converted to float... check the schema to be sure...\n",
        "* https://sparkbyexamples.com/pyspark/pyspark-join-two-or-multiple-dataframes/"
      ],
      "metadata": {
        "id": "ukF0-_U8oQ03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "censusDF = demoDF.join(econDF,[\"ZCTA5\"])\\\n",
        "  .join(housingDF,[\"ZCTA5\"])\\\n",
        "  .join(ruralUrbanDF,[\"ZCTA5\"])\\\n",
        "  .join(socialDF,[\"ZCTA5\"])\n"
      ],
      "metadata": {
        "id": "G-X7ZhBloPMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "censusDF.printSchema()"
      ],
      "metadata": {
        "id": "BHzjWiiXcGdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(censusDF.count(),len(censusDF.columns))"
      ],
      "metadata": {
        "id": "cqhT8zFmT0fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "censusDF = censusDF.withColumnRenamed(\"ZCTA5\",\"IntZips\")\n",
        "censusDF = zipcodesDF.join(censusDF).where(censusDF[\"IntZips\"] == zipcodesDF[\"ZCTA5\"].cast(\"int\")).drop(F.col(\"IntZips\"))\n",
        "censusDF.printSchema()"
      ],
      "metadata": {
        "id": "02aJBEYXcKKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(censusDF.count(),len(censusDF.columns))"
      ],
      "metadata": {
        "id": "xERJK3msuPBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Join the combined census data to the PPP data based on the zipcode "
      ],
      "metadata": {
        "id": "ECCq2PBHiWDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call this new file `df`. Use `df` for all future analysis questions."
      ],
      "metadata": {
        "id": "58t_NGxzp7Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pppDF.join(censusDF, censusDF.ZCTA5 == pppDF.BorrowerZip).drop('ZCTA5')"
      ],
      "metadata": {
        "id": "q3jguTY0ii41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "print(df.count(),len(df.columns))"
      ],
      "metadata": {
        "id": "IODl4LpPANY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "id": "NDEWwF1nAPvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Identify the Top 10 states that received loans"
      ],
      "metadata": {
        "id": "9GCTJvjxijTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined as the sum of funds per State across all records. You will need to recode this column because it is categorical data. Use the upper limit of each range and make sure it is NUMERIC data."
      ],
      "metadata": {
        "id": "7gDoliZwv4gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing sum from pyspark to overide default sum function\n",
        "from pyspark.sql.functions import sum,desc\n",
        "TotalFundsbyState = df.groupBy(\"BorrowerState\") \\\n",
        "  .agg(sum(\"CurrentApprovalAmount\").alias(\"Funds\")) \\\n",
        "  .sort(desc(\"Funds\")) \\\n",
        "  .where(F.col('BorrowerState') != 'null')\n",
        "print(TotalFundsbyState.show(10, truncate = False))\n",
        "print(\"Top 10 States that received funding are\")\n",
        "for x in TotalFundsbyState.select(TotalFundsbyState[\"BorrowerState\"]).collect()[0:10]:\n",
        "  print(x[0])"
      ],
      "metadata": {
        "id": "-2DTwhvqipWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) Identify the Top 10 banks that loaned money\n",
        "Defined as the sum of loaned funds per State across all records. Are many of the banks who loaned the most money in the same State? \n",
        "\n",
        "Again, make sure you are using the numeric version of this categorical column!"
      ],
      "metadata": {
        "id": "P0Iau-sIJGfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "id": "IhcCFacyJMxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing sum from pyspark to overide default sum function\n",
        "from pyspark.sql.functions import sum\n",
        "TotalFundsbyBank = df.groupBy(\"ServicingLenderName\",\"ServicingLenderState\",\"BorrowerState\")\\\n",
        "  .agg(sum(\"CurrentApprovalAmount\").alias(\"Funds\")) \\\n",
        "  .sort(desc(\"Funds\"))\n",
        "print(TotalFundsbyBank.show(10, truncate = False))"
      ],
      "metadata": {
        "id": "1yjo-PNL34uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 10 Banks that loaned money are\")\n",
        "for x in TotalFundsbyBank.select(TotalFundsbyBank[\"ServicingLenderName\"]).collect()[0:10]:\n",
        "  print(x[0])"
      ],
      "metadata": {
        "id": "LwpKiT_U35DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Banks who loaned the money in the same State are {0}\".format(TotalFundsbyBank.filter(TotalFundsbyBank[\"ServicingLenderState\"] == TotalFundsbyBank[\"BorrowerState\"] ).count()))\n",
        "print(\"Number of Banks who loaned the most money are in the same State are {0}\".format(TotalFundsbyBank.limit(10).filter(TotalFundsbyBank[\"ServicingLenderState\"] == TotalFundsbyBank[\"BorrowerState\"] ).count()))"
      ],
      "metadata": {
        "id": "LZAL1zeZ36tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) Which banks loaned the most money to businesses that were in the same State?\n",
        "You will need to do a logical statement here! For example, given that a bank is headquarted in CT, how much money was loaned to CT businesses by this bank?\n",
        "\n",
        "Generalize to all banks and businesses where this is true, then sort from high to low (return the top 10.)"
      ],
      "metadata": {
        "id": "rP4B2AScMFvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Banks who the most money to businesses that were in the same State are\")\n",
        "TotalFundsbyBank.filter(TotalFundsbyBank[\"ServicingLenderState\"] == TotalFundsbyBank[\"BorrowerState\"] ).show(10,truncate = False)"
      ],
      "metadata": {
        "id": "_bg4qzwjMdRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4L2wdm83_Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (5) Which zipcodes received the most funding per capita?\n",
        "This requires you to calculate the sum of funds per zipcode and then divide by the population per zipcode, then sort from high to low. \n",
        "\n",
        "Make sure you use the clean 5 digit zip code!"
      ],
      "metadata": {
        "id": "4a9w5k3LyTkY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDDIwUx3-_Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9NVStAm4GAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (6) What is the top industry (NAICS codes) in the top 100 zip codes that received money?\n",
        "Top 100 zip codes meaning the zip codes that borrowed the most money. I want the most common NAICS code per zip code. Show all 100 rows then sum per NAICS to describe overall trends.\n",
        "\n",
        "This one is tricky!"
      ],
      "metadata": {
        "id": "rNFUWD1vxUg7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RL_jPktlJQGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (7) What are the socioeconomic characteristics of the top 100 zip codes? How do they relate to loan amounts?\n",
        "\n",
        "Select two census variables that look interesting to you. \n",
        "\n",
        "Create histograms or scatterplots of those interesting socioeconomic variables with data from the PPP. Make them beautiful and describe what you see.\n",
        "\n",
        "Don't just pick random variables - try to find an interesting story to tell with data and motivate WHY you picked this column!"
      ],
      "metadata": {
        "id": "apfTMApzwEwp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LxlatTV-xTEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (8) Optional extra credit for +10 points. What are the Top 10 zipcodes that had the highest per capita amount of loans going to Women-Owned businesses?\n",
        "\n",
        "This column is called `Gender` in the `PPP` data.\n",
        "\n",
        "* What State are these zipcodes in? \n",
        "* Comment on if they have anything in common? "
      ],
      "metadata": {
        "id": "9ay33g4BH83C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygBON1OjLM2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔴 Conclusion (5 pts)"
      ],
      "metadata": {
        "id": "ilXXyWRlJwyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) What did you learn? \n",
        "Write five detailed bullets about what you learned. "
      ],
      "metadata": {
        "id": "GgO8oYoNAKSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On Your Own (no credit)\n",
        "No points - but can you download ALL files for PPP data (several gigabytes of data) and re-run this script with ease? Post on the discussion board if you do!"
      ],
      "metadata": {
        "id": "HmBomBoEmyrm"
      }
    }
  ]
}